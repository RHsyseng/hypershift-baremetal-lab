= Cluster Deployment
include::_attributes.adoc[]
:profile: telco-hypershift-baremetal-lab

[#prepare-parameter-file]
== Prepare Parameter file

We prepare a parameter file named `lab_params.yml` providing relevant information for deployment

```
version: stable
tag: 4.13
ingress_ip: 192.168.122.248
workers: 3
sslip: true
baremetal_hosts:
- url: http://192.168.122.1:9000/redfish/v1/Systems/local/lab-0
  mac: "aa:aa:aa:bb:bb:90"
- url: http://192.168.122.1:9000/redfish/v1/Systems/local/lab-1
  mac: "aa:aa:aa:bb:bb:91"
- url: http://192.168.122.1:9000/redfish/v1/Systems/local/lab-2
  mac: "aa:aa:aa:bb:bb:92"
```

In this output, note

* version and tag allow to set specific versions, as long as they are supported by the Hypershift operator
* we specify an ingress vip that will be added to the nodes via machineconfig. An alternative and common way would be to use a loadbalancer ip via metallb
* the baremetal_hosts array contains a list of hosts to be booted via Redfish. For each of them , we specify a bmc url and a valid MAC address for the node so that metal3 operator can identify it. This array would also contain user/password credentials if using real bare metal nodes.

**NOTE:** The boolean flag can be set to assisted to install hypershift and assisted if missing.

[#launch-deployment]
== Launch deployment

We launch the deployment with

```
kcli create cluster hypershift --pf lab_params.yml lab
```

Expected Output

```
Deploying cluster lab
Using default class odf-storagecluster-ceph-rbd
Using 10.19.135.112 as management api ip
Using keepalived virtual_router_id 248
Setting domain to 192-168-122-248.sslip.io
Creating control plane assets
namespace/clusters configured
namespace/clusters-lab created
secret/lab-pull-secret created
secret/lab-ssh-key created
role.rbac.authorization.k8s.io/capi-provider-role created
infraenv.agent-install.openshift.io/lab created
secret/lab-pull-secret created
secret/lab-ssh-key created
hostedcluster.hypershift.openshift.io/lab created
Downloading openshift-install from https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.13
Move downloaded openshift-install somewhere in your PATH if you want to reuse it
Using installer version 4.13.0
secret/lab-node-0 created
baremetalhost.metal3.io/lab-node-0 created
secret/lab-node-1 created
baremetalhost.metal3.io/lab-node-1 created
secret/lab-node-2 created
baremetalhost.metal3.io/lab-node-2 created
Waiting for 3 agents to appear
configmap/assisted-ingress-lab created
nodepool.hypershift.openshift.io/lab created
Warning: would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "autoapprover" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "autoapprover" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "autoapprover" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "autoapprover" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
cronjob.batch/lab-autoapprover created
Waiting for kubeconfig to be available
# kubeconfig
Waiting for kubeadmin-password to be available
# password
Launching install-complete step. It will be retried extra times to handle timeouts
INFO Waiting up to 40m0s (until 3:54PM) for the cluster at https://10.19.135.112:30155 to initialize...
INFO Checking to see if there is a route at openshift-console/console...
INFO Install complete!
INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/root/.kcli/clusters/lab/auth/kubeconfig'
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.lab.192-168-122-248.sslip.io
INFO Login to the console with user: "kubeadmin", and password: "XXXX-YYYYY-ZZZ-WWWW\n"
INFO Time elapsed: 21m42s
```

This will:

* Create a hostedcluster object
* Create an infraenv object
* Create baremetal host objects (bmh) using the spec from the parameter file. The vms will be booted via redfish as would bare metal nodes.
* Wait for the corresponding nodes to boot and register as agents
* Download locally openshift-install to properly evaluate the release image to use in the nodepool spec
* Create a nodepool object with the corresponding number of replicas and the correct release image
* Wait for the installation to complete using openshift-install `wait-for install-complete` subcommand

[#check-control-plane]
== Check Control Plane

When using hypershift, the control planes component are hosted in a dedicated namespace, as we can see with

```
oc get pod -n clusters-lab
```

Expected Output

```
NAME                                                  READY   STATUS    RESTARTS   AGE
capi-provider-67c67c9c4f-vvxgh                        1/1     Running   0          57m
catalog-operator-76d68cf889-wxc7b                     2/2     Running   0          55m
certified-operators-catalog-686984f5cb-xgnsq          1/1     Running   0          55m
cluster-api-c9c66b697-57c6x                           1/1     Running   0          57m
cluster-autoscaler-9bb9cfd97-tbns7                    1/1     Running   0          56m
cluster-image-registry-operator-77fd45fc44-wfclb      2/2     Running   0          55m
cluster-network-operator-5b5b464b6c-pc6kj             1/1     Running   0          55m
cluster-node-tuning-operator-d5799f99c-6dnbk          1/1     Running   0          55m
cluster-policy-controller-5595cbc764-95wcn            1/1     Running   0          55m
cluster-storage-operator-dd85cdf45-bcqjn              1/1     Running   0          55m
cluster-version-operator-56c45796b9-bdgj6             1/1     Running   0          55m
community-operators-catalog-6d47696f8-cflfb           1/1     Running   0          55m
control-plane-operator-664cd8878b-mp95v               1/1     Running   0          57m
csi-snapshot-controller-7d89bf444-rg87x               1/1     Running   0          54m
csi-snapshot-controller-operator-595dcb54bc-2q9p2     1/1     Running   0          55m
csi-snapshot-webhook-8c945d4f9-zcsq2                  1/1     Running   0          54m
dns-operator-59487fbbdf-nd428                         1/1     Running   0          55m
etcd-0                                                2/2     Running   0          56m
hosted-cluster-config-operator-d67b4d585-ng2g6        1/1     Running   0          55m
ignition-server-54d5f7785d-tr9xs                      1/1     Running   0          56m
ingress-operator-577dbfd585-fxlbb                     2/2     Running   0          55m
konnectivity-agent-84cd6b7875-9bqsk                   1/1     Running   0          56m
konnectivity-server-7c94cbd9f-d8zs8                   1/1     Running   0          56m
kube-apiserver-5c787c5f59-7hl2t                       3/3     Running   0          56m
kube-controller-manager-7d54cdcc47-4kshc              1/1     Running   0          25m
kube-scheduler-55c4c757f6-5gn8h                       1/1     Running   0          55m
machine-approver-7fd7f47c5f-s7ftq                     1/1     Running   0          56m
multus-admission-controller-6d5459f886-c286m          2/2     Running   0          30m
oauth-openshift-5864b48666-p8bt7                      2/2     Running   0          54m
olm-operator-74cd7b96-krnbf                           2/2     Running   0          55m
openshift-apiserver-7679468b7f-rzrrp                  3/3     Running   0          25m
openshift-controller-manager-796f49bf74-tdcrs         1/1     Running   0          55m
openshift-oauth-apiserver-7f586b5c88-6ggmz            2/2     Running   0          55m
openshift-route-controller-manager-858f67d7b5-6ss7p   1/1     Running   0          55m
ovnkube-master-0                                      7/7     Running   0          30m
packageserver-65cc888f58-llsb7                        2/2     Running   0          55m
redhat-marketplace-catalog-767447c99b-dmsqj           1/1     Running   0          55m
redhat-operators-catalog-88df6b978-5rl2n              1/1     Running   0          39m
```

In this output, note the `capi-provider` pod which is in charge of assigning agents to a hosted cluster when replicas of a nodepool are indicated

[#check-assisted-components]
== Check Assisted Components

During deployment, we created baremetal hosts object, which were annotated with a label `infraenvs.agent-install.openshift.io: lab`

For instance, we can see it the bmh associated to lab-node-0

```
oc get bmh -n clusters-lab lab-node-0 -o yaml
```

Expected Output

```
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: lab-node-1
  namespace: clusters-lab
  labels:
    infraenvs.agent-install.openshift.io: lab
  annotations:
    inspect.metal3.io: disabled
    bmac.agent-install.openshift.io/hostname: lab-node-1
    bmac.agent-install.openshift.io/role: worker
spec:
  bmc:
    disableCertificateVerification: True
    address: redfish-virtualmedia+http://192.168.122.1:9000/redfish/v1/Systems/local/lab-1
    credentialsName: lab-node-1
  bootMACAddress: aa:aa:aa:bb:bb:91
  hardwareProfile: unknown
  online: true
  automatedCleaningMode: disabled
  bootMode: legacy
```

With this annotation, the corresponding nodes are booted via Redfish with an iso that makes them available as part of the infraenv

```
oc get agent -n clusters-lab
```

Expected Output

```
NAME                                   CLUSTER   APPROVED   ROLE     STAGE
0d711921-1afd-42e8-b2af-59b1e24d1b62   lab       true       worker   Done
a1b35081-b4ec-4569-aff2-db0075eb8df2   lab       true       worker   Done
e26fc0e1-9fd4-42c2-8959-7ff9acb8fe8f   lab       true       worker   Done
```

When the replicas number in the nodepool object gets changed, capi-provider component tries to locate available agent and plug them as additional workers to the corresponding cluster

[#accessing-cluster]
== Accessing the cluster

The kubeconfig corresponding to our installation gets stored in `$HOME/.kcli/clusters/lab/auth/kubeconfig` but we can also retrieve it manually using the following command:

```
CLUTSTER=lab
oc extract -n clusters secret/$CLUSTER-admin-kubeconfig --to=- > kubeconfig.$CLUSTER
```

Expected output

```
# kubeconfig
```

With the kubeconfig, we can check how the installation is successful

* By Checking the nodes

```
export KUBECONFIG=$HOME/.kcli/clusters/lab/auth/kubeconfig
oc get nodes
```

Expected output

```
NAME         STATUS   ROLES    AGE     VERSION
lab-node-0   Ready    worker   6m37s   v1.25.7+eab9cc9
lab-node-1   Ready    worker   6m35s   v1.25.7+eab9cc9
lab-node-2   Ready    worker   5m35s   v1.25.7+eab9cc9
```

* By Checking the version of the cluster

```
export KUBECONFIG=$HOME/.kcli/clusters/lab/auth/kubeconfig
oc get clusterversion
```

Expected output

```
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.13.0   True        False         7m8s    Cluster version is 4.13.0
```

* By Checking the cluster operators

```
export KUBECONFIG=$HOME/.kcli/clusters/lab/auth/kubeconfig
oc get co
```

Expected output

```
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
console                                    4.13.0   True        False         False      14m
csi-snapshot-controller                    4.13.0   True        False         False      42m
dns                                        4.13.0   True        False         False      12m
image-registry                             4.13.0   True        False         False      12m
ingress                                    4.13.0   True        False         False      41m
insights                                   4.13.0   True        False         False      17m
kube-apiserver                             4.13.0   True        False         False      42m
kube-controller-manager                    4.13.0   True        False         False      42m
kube-scheduler                             4.13.0   True        False         False      42m
kube-storage-version-migrator              4.13.0   True        False         False      17m
monitoring                                 4.13.0   True        False         False      15m
network                                    4.13.0   True        False         False      12m
node-tuning                                4.13.0   True        False         False      18m
openshift-apiserver                        4.13.0   True        False         False      42m
openshift-controller-manager               4.13.0   True        False         False      42m
openshift-samples                          4.13.0   True        False         False      16m
operator-lifecycle-manager                 4.13.0   True        False         False      41m
operator-lifecycle-manager-catalog         4.13.0   True        False         False      41m
operator-lifecycle-manager-packageserver   4.13.0   True        False         False      42m
service-ca                                 4.13.0   True        False         False      17m
storage                                    4.13.0   True        False         False      41m
```
