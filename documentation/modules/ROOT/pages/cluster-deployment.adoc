= Cluster Deployment
include::_attributes.adoc[]
:profile: telco-hypershift-baremetal-lab

In this section, we will deploy the assets needed to create an Openshift Cluster and launch baremetal like vms that will end up as the workers of our cluster

[#create-hosted-cluster]
== Create Hosted Cluster

We will render a yaml such as the following one where we note:

* We create a base namespace `clusters` where base assets will be created.
* The control plane pods will be running in clusters-$CLUSTER namespace
* `CLUSTER` and `DOMAIN` will need to have the proper associated wildcard DNS entry for app.$CLUSTER.$DOMAIN
* `RELEASE_IMAGE` comes from openshift-install binary, to match desired target version.
* `MACHINE_CIDR` corresponds to the cidr of the nodes we will be adding as workers.

NOTE: In our case, we will be using a vip set up through metallb and to ease DNS, sslip service which resolves an fqdn such as 192-168-122-252.sslip.io to the corresponding ip 192.168.122.252

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
apiVersion: v1
data:
 .dockerconfigjson: ${PULL_SECRET}
kind: Secret
metadata:
  name: ${CLUSTER}-pull-secret
  namespace: clusters
type: kubernetes.io/dockerconfigjson
---
apiVersion: v1
kind: Secret
metadata:
  name: ${CLUSTER}-ssh-key
  namespace: clusters
stringData:
  id_rsa.pub: ${SSH_PUB_KEY}
---
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: ${CLUSTER}
  namespace: clusters
spec:
  release:
    image: ${RELEASE_IMAGE}
  dns:
    baseDomain: ${DOMAIN}
  etcd:
    managed:
      storage:
        persistentVolume:
          size: 4Gi
        restoreSnapshotURL: null
        type: PersistentVolume
    managementType: Managed
  pullSecret:
    name: ${CLUSTER}-pull-secret
  sshKey:
    name: ${CLUSTER}-ssh-key
  fips: False
  networking:
    clusterNetwork:
    - cidr: 10.129.0.0/14
      hostPrefix: 23
    serviceNetwork:
    - cidr: 172.31.0.0/16
    machineNetwork:
    - cidr: ${MACHINE_CIDR}
    networkType: OVNKubernetes
  platform:
    agent:
      agentNamespace: clusters-${CLUSTER}
    type: Agent
  infraID: ${CLUSTER}
  dns:
    baseDomain: ${DOMAIN}
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: ${API_IP}
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      type: Route
  - service: OIDC
    servicePublishingStrategy:
      type: Route
  - service: Konnectivity
    servicePublishingStrategy:
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
  - service: OVNSbDb
    servicePublishingStrategy:
      type: Route
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
export CLUSTER=myhypershift 
export DOMAIN=192-168-122-252.sslip.io
export PULL_SECRET=$(cat ~/openshift_pull.json | tr -d [:space:]| base64 -w0)
export SSH_PUB_KEY=$(cat ~/.ssh/id_rsa.pub)
export RELEASE_IMAGE=$(openshift-install version | grep image | awk -F ' ' '{print $3}')
export MACHINE_CIDR="192.168.122.0/24"
export API_IP=$(oc get nodes --no-headers -o jsonpath="{.items[*]['status.addresses'][0]['address']}" | head -1)
oc create namespace clusters
envsubst < scripts/HOSTEDCLUSTER/hostedcluster.yaml.sample | oc create -f -
-----

Expected Output

```
secret/myhypershift-ssh-key created
secret/myhypershift-pull-secret created
hostedcluster.hypershift.openshift.io/myhypershift created
```

[#check-control-plane]
== Check Control Plane

After waiting a bit, the control planes show up

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc get pod -n clusters-$CLUSTER
-----

Expected output

```
NAME                                                  READY   STATUS    RESTARTS   AGE
capi-provider-668b8c4766-sq5r9                        1/1     Running   0          7m39s
catalog-operator-64dbc4fd47-nltck                     2/2     Running   0          6m4s
certified-operators-catalog-55658d9d49-w88wl          1/1     Running   0          6m1s
cluster-api-7476cd5486-rjg52                          1/1     Running   0          7m39s
cluster-autoscaler-67d4dd7df5-l77qr                   1/1     Running   0          7m13s
cluster-image-registry-operator-5846c8c7d8-lwsrc      2/2     Running   0          6m4s
cluster-network-operator-84f8d7c568-5w5n8             1/1     Running   0          6m6s
cluster-node-tuning-operator-6d4455cd79-lwztq         1/1     Running   0          6m6s
cluster-policy-controller-55c5d878c9-fdfxd            1/1     Running   0          6m6s
cluster-storage-operator-7fb9c6cd58-bbnbr             1/1     Running   0          6m4s
cluster-version-operator-6fbd885478-wlf54             1/1     Running   0          6m6s
community-operators-catalog-7bc7bfd5d-bz5lt           1/1     Running   0          6m1s
control-plane-operator-5ddb694874-mdfxc               1/1     Running   0          7m39s
csi-snapshot-controller-5bcfdb77b8-ks9mv              1/1     Running   0          5m32s
csi-snapshot-controller-operator-cf95f6449-ffnqv      1/1     Running   0          6m4s
csi-snapshot-webhook-66c8f6489f-4fmvt                 1/1     Running   0          5m32s
dns-operator-77b55fc876-xmwbj                         1/1     Running   0          6m5s
etcd-0                                                2/2     Running   0          7m14s
hosted-cluster-config-operator-5bd97744bf-w25rz       1/1     Running   0          6m5s
ignition-server-6bbb7846f-9b5fp                       1/1     Running   0          7m14s
ingress-operator-7f9c4fd895-kvtqx                     2/2     Running   0          6m5s
konnectivity-agent-b747c65c4-z95xf                    1/1     Running   0          7m13s
konnectivity-server-7b767d95d-zbj9f                   1/1     Running   0          7m15s
kube-apiserver-6d8fdd78d-l7nz6                        3/3     Running   0          7m14s
kube-controller-manager-6b94f8f6bb-s8xf7              1/1     Running   0          5m29s
kube-scheduler-54b99b9f94-rt6gv                       1/1     Running   0          6m14s
machine-approver-7d7d48b59f-k8q2m                     1/1     Running   0          7m13s
oauth-openshift-fdf657785-xw28k                       2/2     Running   0          5m26s
olm-operator-56bb647d76-x5flg                         2/2     Running   0          6m4s
openshift-apiserver-869fb7779d-lvhbt                  3/3     Running   0          5m29s
openshift-controller-manager-5f7c556569-shqm9         1/1     Running   0          6m6s
openshift-oauth-apiserver-566d85b87c-ng7nv            2/2     Running   0          6m6s
openshift-route-controller-manager-5d787bcb8b-xsk2v   1/1     Running   0          6m6s
packageserver-6767c68984-l92ln                        2/2     Running   0          6m4s
redhat-marketplace-catalog-594d77cfd-lmqt5            1/1     Running   0          6m1s
redhat-operators-catalog-8647b7fd55-m2dft             1/1     Running   0          6m1s
```

In this output, note the `capi-provider` pod which is in charge of assigning agents to a hosted cluster when replicas of a nodepool are indicated

We can also check the status of the control plane by checking the hostedcluster object

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc get hc -n clusters $CLUSTER
-----

Expected output

```
NAME           VERSION   KUBECONFIG                      PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
myhypershift             myhypershift-admin-kubeconfig   Partial    True        False         The hosted control plane is available
```

We can already gather the KUBECONFIG corresponding to our cluster by running the following command

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc extract -n clusters  secret/$CLUSTER-admin-kubeconfig --to=- > kubeconfig.$CLUSTER
-----

[#create-bmhs]
== Create Bare Metal Hosts

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
apiVersion: v1
data:
  password: ${PASSWORD}
  username: ${USER}
kind: Secret
metadata:
  name: $NODE
  namespace: clusters-${CLUSTER}
type: Opaque
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: ${NODE}
  namespace: clusters-$CLUSTER
  labels:
    infraenvs.agent-install.openshift.io: ${CLUSTER}
  annotations:
    inspect.metal3.io: disabled
    bmac.agent-install.openshift.io/hostname: ${NODE}
    bmac.agent-install.openshift.io/role: worker
spec:
  bmc:
    disableCertificateVerification: True
    address: ${URL}
    credentialsName: ${NODE}
  bootMACAddress: ${MAC}
  hardwareProfile: unknown
  online: true
  automatedCleaningMode: disabled
  bootMode: legacy
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
export CLUSTER=myhypershift
export USER=$(echo -n root| base64 -w0)
export PASSWORD=$(echo -n root| base64 -w0)
export NODE=lab-assisted-0
export MAC=aa:aa:aa:bb:bb:90
export URL=redfish-virtualmedia+http://192.168.122.1:9000/redfish/v1/Systems/local/lab-assisted-0
envsubst < scripts/BMH/bmh.yaml.sample | oc create -f -

export NODE=lab-assisted-1
export MAC=aa:aa:aa:bb:bb:91
export URL=redfish-virtualmedia+http://192.168.122.1:9000/redfish/v1/Systems/local/lab-assisted-1
envsubst < scripts/BMH/bmh.yaml.sample | oc create -f -
-----

Expected Output

```
TODO
```

The nodes will boot via redfish and my mean of the infraenv label do so using the ISO associated to our environment

After a while, they will appear as available agents in the clusters-$CLUSTER namespace

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc get agent -n clusters-$CLUSTER
-----

Expected Output

```
NAME                                   CLUSTER   APPROVED   ROLE     STAGE
2b2ec244-0a78-4289-8ba9-0ba4f956535c             true       worker
ed60a67f-4d9b-4ac4-bbc8-be119c832859             true       worker
```

[#create-nodepool]
== Create Node Pool

Once agents ara available, we can create a nodepool to actually install those nodes as workers.

Note that in this case, we will use the same target version for this group of workers, although this could perfectly be a different one

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  name: ${CLUSTER}
  namespace: clusters
spec:
  clusterName: ${CLUSTER}
  management:
    autoRepair: false
    upgradeType: InPlace
  platform:
    type: Agent
  release:
    image: ${RELEASE_IMAGE}
  replicas: 2
status:
  conditions: null
  replicas: 2
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
export CLUSTER=myhypershift 
export RELEASE_IMAGE=$(openshift-install version | grep image | awk -F ' ' '{print $3}')
envsubst < scripts/HOSTEDCLUSTER/nodepool.yaml.sample | oc create -f -
-----

Expected output

```
nodepool.hypershift.openshift.io/myhypershift created
```

When the replicas number in the nodepool object gets changed, capi-provider component tries to locate available agent and plug them as additional workers to the corresponding cluster

After a while, the nodes will actually be part of our cluster, which is ready for regular use.

We can monitor progress with the following command

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
CLUSTER=myhypershift 
oc get nodepool -n clusters $CLUSTER
-----

Expected output

```
NAME           CLUSTER        DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
myhypershift   myhypershift   2                               False         False        4.13.0                                       0 of 2 completed
```
